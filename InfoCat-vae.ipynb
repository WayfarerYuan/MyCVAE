{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 129609.62735493977, BCE: 129529.49825541179, KLD: 80.12901411758503\n",
      "Epoch: 2, Loss: 97400.51867167155, BCE: 97316.09371948242, KLD: 84.42494252324104\n",
      "Epoch: 3, Loss: 96926.60726420085, BCE: 96842.18118286133, KLD: 84.42667317887147\n",
      "Epoch: 4, Loss: 96825.89423116048, BCE: 96741.46793619792, KLD: 84.42725592354934\n",
      "Epoch: 5, Loss: 96786.72950744629, BCE: 96702.3031056722, KLD: 84.4276611705621\n",
      "Epoch: 6, Loss: 96770.15310668945, BCE: 96685.72667439778, KLD: 84.42778872450192\n",
      "Epoch: 7, Loss: 96759.43170674641, BCE: 96675.00527445476, KLD: 84.42791796227296\n",
      "Epoch: 8, Loss: 96760.5835164388, BCE: 96676.15708414714, KLD: 84.42794930438201\n",
      "Epoch: 9, Loss: 96750.33815002441, BCE: 96665.91171773274, KLD: 84.42801649371783\n",
      "Epoch: 10, Loss: 96751.30640157063, BCE: 96666.87996927898, KLD: 84.42804500460625\n",
      "Epoch: 11, Loss: 96747.79201253255, BCE: 96663.36558024089, KLD: 84.42805350820224\n",
      "Epoch: 12, Loss: 96750.39926656087, BCE: 96665.97283426921, KLD: 84.42807715634505\n",
      "Epoch: 13, Loss: 96748.95809936523, BCE: 96664.53166707356, KLD: 84.42809878786404\n",
      "Epoch: 14, Loss: 96742.31903076172, BCE: 96657.89259847005, KLD: 84.42809999982516\n",
      "Epoch: 15, Loss: 96746.4985148112, BCE: 96662.07208251953, KLD: 84.42810123662154\n",
      "Epoch: 16, Loss: 96747.13773600261, BCE: 96662.71130371094, KLD: 84.42810257275899\n",
      "Epoch: 17, Loss: 96745.20703125, BCE: 96660.78059895833, KLD: 84.42810739080112\n",
      "Epoch: 18, Loss: 96743.08847045898, BCE: 96658.66203816731, KLD: 84.42811303834121\n",
      "Epoch: 19, Loss: 96741.59406026204, BCE: 96657.16762797038, KLD: 84.42811412612598\n",
      "Epoch: 20, Loss: 96745.13656616211, BCE: 96660.71013387044, KLD: 84.42811501522858\n",
      "Epoch: 21, Loss: 96746.47580973308, BCE: 96662.0493774414, KLD: 84.42811509470145\n",
      "Epoch: 22, Loss: 96743.89381917317, BCE: 96659.46738688152, KLD: 84.42811940113704\n",
      "Epoch: 23, Loss: 96744.36824035645, BCE: 96659.94180806477, KLD: 84.42812370757262\n",
      "Epoch: 24, Loss: 96742.08012898763, BCE: 96657.65369669597, KLD: 84.42812371750672\n",
      "Epoch: 25, Loss: 96744.46837361653, BCE: 96660.04194132487, KLD: 84.42812396089236\n",
      "Epoch: 26, Loss: 96744.2796681722, BCE: 96659.85323588054, KLD: 84.42812412480514\n",
      "Epoch: 27, Loss: 89098.29679870605, BCE: 89020.58610534668, KLD: 77.71225096285343\n",
      "Epoch: 28, Loss: 96741.07853190105, BCE: 96656.65209960938, KLD: 84.42812464634578\n",
      "Epoch: 29, Loss: 96741.8863627116, BCE: 96657.45993041992, KLD: 84.42812482515971\n",
      "Epoch: 30, Loss: 96741.96117655437, BCE: 96657.5347442627, KLD: 84.42812524239223\n",
      "Epoch: 31, Loss: 96738.06355285645, BCE: 96653.63712056477, KLD: 84.42812539140384\n",
      "Epoch: 32, Loss: 96742.33469136556, BCE: 96657.9082590739, KLD: 84.42812568446\n",
      "Epoch: 33, Loss: 96741.16454060872, BCE: 96656.73810831706, KLD: 84.42812598745029\n",
      "Epoch: 34, Loss: 96741.1168314616, BCE: 96656.69039916992, KLD: 84.42812656859557\n",
      "Epoch: 35, Loss: 96739.71588643391, BCE: 96655.28945414226, KLD: 84.4281269411246\n",
      "Epoch: 36, Loss: 96741.75272115071, BCE: 96657.32628885905, KLD: 84.42812735339005\n",
      "Epoch: 37, Loss: 96739.53610738118, BCE: 96655.10967508952, KLD: 84.42812782029311\n",
      "Epoch: 38, Loss: 96738.85028584798, BCE: 96654.42385355632, KLD: 84.42812798420589\n",
      "Epoch: 39, Loss: 96738.7739054362, BCE: 96654.34747314453, KLD: 84.42812804381053\n",
      "Epoch: 40, Loss: 96737.01754252116, BCE: 96652.59111022949, KLD: 84.42812810341518\n",
      "Epoch: 41, Loss: 96739.64560953777, BCE: 96655.2191772461, KLD: 84.4281281332175\n",
      "Epoch: 42, Loss: 96736.4853515625, BCE: 96652.05891927083, KLD: 84.42812799910705\n",
      "Epoch: 43, Loss: 96736.82768758138, BCE: 96652.40125528972, KLD: 84.42812843124072\n",
      "Epoch: 44, Loss: 96737.15131632487, BCE: 96652.7248840332, KLD: 84.42812871436278\n",
      "Epoch: 45, Loss: 96739.47521972656, BCE: 96655.04878743489, KLD: 84.42812939484914\n",
      "Epoch: 46, Loss: 96737.95520528157, BCE: 96653.52877298991, KLD: 84.42812929550807\n",
      "Epoch: 47, Loss: 96739.91749572754, BCE: 96655.49106343587, KLD: 84.42813007036845\n",
      "Epoch: 48, Loss: 96738.0345102946, BCE: 96653.60807800293, KLD: 84.42813042799632\n",
      "Epoch: 49, Loss: 96734.9136912028, BCE: 96650.48725891113, KLD: 84.42813017467658\n",
      "Epoch: 50, Loss: 96738.39567565918, BCE: 96653.96924336751, KLD: 84.42812974254291\n",
      "Epoch: 51, Loss: 96735.6727701823, BCE: 96651.24633789062, KLD: 84.42812845607598\n",
      "Epoch: 52, Loss: 96735.45578511556, BCE: 96651.02938334148, KLD: 84.42808779080708\n",
      "Epoch: 53, Loss: 96733.10293070476, BCE: 96648.67649841309, KLD: 84.42812674740951\n",
      "Epoch: 54, Loss: 96737.93999735515, BCE: 96653.51356506348, KLD: 84.42812469104926\n",
      "Epoch: 55, Loss: 93698.89853922527, BCE: 93619.62557983398, KLD: 79.27364527185757\n",
      "Epoch: 56, Loss: 87092.0077311198, BCE: 87016.9437713623, KLD: 75.06403129796188\n",
      "Epoch: 57, Loss: 84248.28535461426, BCE: 84177.27783203125, KLD: 71.0074552198251\n",
      "Epoch: 58, Loss: 81841.50296529134, BCE: 81768.33852132161, KLD: 73.16443496445815\n",
      "Epoch: 59, Loss: 80784.82916768391, BCE: 80710.0178120931, KLD: 74.81134932736556\n",
      "Epoch: 60, Loss: 80349.41266886394, BCE: 80274.20139567058, KLD: 75.21125728388627\n",
      "Epoch: 61, Loss: 80194.1302134196, BCE: 80118.75638834636, KLD: 75.37375771999359\n",
      "Epoch: 62, Loss: 80118.51055399577, BCE: 80043.01349894206, KLD: 75.4969589014848\n",
      "Epoch: 63, Loss: 80083.26614888509, BCE: 80007.81835428874, KLD: 75.44763372341792\n",
      "Epoch: 64, Loss: 80067.35830688477, BCE: 79992.05167134602, KLD: 75.30655300617218\n",
      "Epoch: 65, Loss: 80055.72383117676, BCE: 79980.40232340495, KLD: 75.32120513916016\n",
      "Epoch: 66, Loss: 80028.06539916992, BCE: 79952.59388224284, KLD: 75.47168445587158\n",
      "Epoch: 67, Loss: 80041.6737721761, BCE: 79966.21012878418, KLD: 75.46380837758382\n",
      "Epoch: 68, Loss: 80032.85824076335, BCE: 79957.29954020183, KLD: 75.55878213544686\n",
      "Epoch: 69, Loss: 80024.82437133789, BCE: 79949.12097676595, KLD: 75.70338373879592\n",
      "Epoch: 70, Loss: 80014.40214538574, BCE: 79938.74401346843, KLD: 75.65798001984756\n",
      "Epoch: 71, Loss: 80001.07336425781, BCE: 79925.41648864746, KLD: 75.65681022405624\n",
      "Epoch: 72, Loss: 79981.67049153645, BCE: 79905.83643595378, KLD: 75.83406037588914\n",
      "Epoch: 73, Loss: 79966.87455749512, BCE: 79890.90751647949, KLD: 75.96687119702499\n",
      "Epoch: 74, Loss: 79941.09356180827, BCE: 79865.09718322754, KLD: 75.99624096353848\n",
      "Epoch: 75, Loss: 79865.1081797282, BCE: 79790.01049804688, KLD: 75.09761699537437\n",
      "Epoch: 76, Loss: 79336.11092631023, BCE: 79261.67628479004, KLD: 74.43471377094586\n",
      "Epoch: 77, Loss: 79245.8082377116, BCE: 79170.71034749348, KLD: 75.0979859183232\n",
      "Epoch: 78, Loss: 79212.7147878011, BCE: 79137.22920227051, KLD: 75.4855207701524\n",
      "Epoch: 79, Loss: 79202.7807413737, BCE: 79127.11489359538, KLD: 75.66586996118228\n",
      "Epoch: 80, Loss: 79149.16458129883, BCE: 79074.45709737141, KLD: 74.707507421573\n",
      "Epoch: 81, Loss: 78879.6724395752, BCE: 78805.61465454102, KLD: 74.05777442455292\n",
      "Epoch: 82, Loss: 78696.16454569499, BCE: 78621.38139851888, KLD: 74.78308598697186\n",
      "Epoch: 83, Loss: 78540.35569763184, BCE: 78465.12727864583, KLD: 75.22843430439632\n",
      "Epoch: 84, Loss: 78451.20286051433, BCE: 78375.86726379395, KLD: 75.33557371795177\n",
      "Epoch: 85, Loss: 78352.9259185791, BCE: 78277.25620524089, KLD: 75.66963900625706\n",
      "Epoch: 86, Loss: 78258.85977681477, BCE: 78182.94140116374, KLD: 75.91825211048126\n",
      "Epoch: 87, Loss: 78183.57043457031, BCE: 78107.43071492513, KLD: 76.13975714147091\n",
      "Epoch: 88, Loss: 78147.29793802898, BCE: 78071.14873758952, KLD: 76.14918253322442\n",
      "Epoch: 89, Loss: 78108.67316691081, BCE: 78032.35693868001, KLD: 76.31633049746354\n",
      "Epoch: 90, Loss: 78096.36509195964, BCE: 78019.95356241862, KLD: 76.41153081258138\n",
      "Epoch: 91, Loss: 11691.021926879883, BCE: 11679.656295776367, KLD: 11.365664318203926\n",
      "Epoch: 92, Loss: 78096.45458475749, BCE: 78020.08469136556, KLD: 76.36993825435638\n",
      "Epoch: 93, Loss: 78097.9595489502, BCE: 78021.37580362956, KLD: 76.58383955061436\n",
      "Epoch: 94, Loss: 78078.34488423665, BCE: 78001.72014363606, KLD: 76.6247155169646\n",
      "Epoch: 95, Loss: 78094.43959554036, BCE: 78017.84533691406, KLD: 76.5943020482858\n",
      "Epoch: 96, Loss: 78069.00069681804, BCE: 77992.39529927571, KLD: 76.60562553505103\n",
      "Epoch: 97, Loss: 77997.2574412028, BCE: 77921.63557942708, KLD: 75.62177655597527\n",
      "Epoch: 98, Loss: 77754.21568806966, BCE: 77679.59245808919, KLD: 74.62328447401524\n",
      "Epoch: 99, Loss: 77602.96346537273, BCE: 77527.7919108073, KLD: 75.17155722777049\n",
      "Epoch: 100, Loss: 77513.88512674968, BCE: 77438.46688334148, KLD: 75.41818084319432\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[28, 28]' is invalid for input of size 7840",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m# Generate average digits 0-9\u001b[39;00m\n\u001b[1;32m    119\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meye(\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 120\u001b[0m generate_digit(model, z)\n",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m, in \u001b[0;36mgenerate_digit\u001b[0;34m(model, z)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_digit\u001b[39m(model, z):\n\u001b[1;32m     87\u001b[0m     digit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecoder(z)\n\u001b[0;32m---> 88\u001b[0m     digit_reshaped \u001b[39m=\u001b[39m digit\u001b[39m.\u001b[39;49mview(\u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     89\u001b[0m     plt\u001b[39m.\u001b[39mimshow(digit_reshaped, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     90\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[28, 28]' is invalid for input of size 7840"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class InfoVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, init_temperature, anneal_rate):\n",
    "        super(InfoVAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)  # Output logits\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = init_temperature\n",
    "        self.anneal_rate = anneal_rate\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        # Gumbel-Softmax reparameterization\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        y = logits + gumbels\n",
    "        return F.softmax(y / self.temperature, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.encoder(x)\n",
    "        z = self.reparameterize(logits)\n",
    "        return self.decoder(z), logits\n",
    "\n",
    "    def anneal_temperature(self):\n",
    "        self.temperature *= self.anneal_rate\n",
    "\n",
    "def compute_loss(recon_x, x, logits, latent_dim):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    q_y = F.softmax(logits, dim=-1)\n",
    "    log_q_y = torch.log(q_y + 1e-20)\n",
    "    KLD = torch.sum(q_y * (log_q_y - torch.log(torch.tensor(1.0 / latent_dim))), dim=-1).mean()*latent_dim\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "def train(model, dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    BCE_loss = 0\n",
    "    KLD_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        data = data.view(-1, 784).to(device)  # Flatten the images\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_batch, logits = model(data)\n",
    "        try:\n",
    "            loss, BCE, KLD = compute_loss(recon_batch, data, logits, model.latent_dim)\n",
    "        except:\n",
    "            break\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() / len(data)\n",
    "        BCE_loss += BCE.item() / len(data)\n",
    "        KLD_loss += KLD.item() / len(data)\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss, BCE_loss, KLD_loss\n",
    "\n",
    "def get_data_loader(dataset_name, transform, batch_size):\n",
    "    if dataset_name == 'MNIST':\n",
    "        dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset name')\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "def generate_digit(model, z):\n",
    "    digit = model.decoder(z)\n",
    "    digit_reshaped = digit.view(28, 28).detach().cpu()\n",
    "    plt.imshow(digit_reshaped, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Run the main function on MNIST\n",
    "dataset_name = 'MNIST'\n",
    "input_dim = 784  # 28*28, size of MNIST images\n",
    "hidden_dim = 32\n",
    "latent_dim = 10\n",
    "init_temperature = 1.0\n",
    "anneal_rate = 0.99\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = InfoVAE(input_dim, hidden_dim, latent_dim, init_temperature, anneal_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Prepare the data\n",
    "transform = transforms.ToTensor()\n",
    "dataloader = get_data_loader(dataset_name, transform, batch_size)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    try:\n",
    "        total_loss, BCE_loss, KLD_loss = train(model, dataloader, optimizer, epoch)\n",
    "        model.anneal_temperature()\n",
    "        print('Epoch: {}, Loss: {}, BCE: {}, KLD: {}'.format(epoch, total_loss, BCE_loss, KLD_loss))\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "# Generate average digits 0-9\n",
    "z = torch.eye(10).to(device)\n",
    "generate_digit(model, z)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
