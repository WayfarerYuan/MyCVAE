{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from prodigyopt import Prodigy\n",
    "import torch.profiler\n",
    "\n",
    "# 定义模型\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, categorical_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, categorical_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, categorical_dim, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(categorical_dim, hidden_dim_2)\n",
    "        self.fc2 = nn.Linear(hidden_dim_2, hidden_dim_1)\n",
    "        self.fc3 = nn.Linear(hidden_dim_1, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # 确保输出在[0,1]范围内\n",
    "        return x\n",
    "\n",
    "def gumbel_softmax(logits, temperature):\n",
    "    gs = F.gumbel_softmax(logits, tau=temperature, hard=True, eps=1e-10, dim=-1)\n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature):\n",
    "    # 加载数据\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "    if dataset_name == \"MNIST\":\n",
    "        dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    elif dataset_name == \"FashionMNIST\":\n",
    "        dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "    elif dataset_name == \"EMNIST\":\n",
    "        dataset = datasets.EMNIST('./data', train=True, download=True, transform=transform, split='balanced')\n",
    "    dataset_img_size = dataset[0][0].shape[0]\n",
    "    # get the number of classes\n",
    "    classes = []\n",
    "    for _, label in dataset:\n",
    "        if label not in classes:\n",
    "            classes.append(label)\n",
    "    num_classes = len(classes)\n",
    "    categorical_dim = num_classes\n",
    "    input_dim = output_dim = dataset_img_size\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    annealing_rate = (temperature - final_temperature) / (epochs * len(dataloader))\n",
    "\n",
    "    # 创建模型\n",
    "    encoder = Encoder(input_dim, hidden_dim_1, hidden_dim_2, categorical_dim)\n",
    "    decoder = Decoder(categorical_dim, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    # optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=initial_lr)\n",
    "    optimizer = Prodigy(list(encoder.parameters()) + list(decoder.parameters()), lr=initial_lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=(len(dataloader) * epochs / 4), eta_min=1e-5)\n",
    "\n",
    "    # initialize the loss lists\n",
    "    recon_losses = []\n",
    "    kl_losses = []\n",
    "    total_losses = []\n",
    "    lr_list = []\n",
    "    temperature_list = []\n",
    "\n",
    "    # initialize the progress bar\n",
    "    progress_bar = tqdm(total=(len(dataloader) * epochs), desc=\"Training Progress\")\n",
    "\n",
    "    # train the model\n",
    "    with torch.profiler.profile(\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    "    ) as profiler:\n",
    "        for epoch in range(epochs):\n",
    "            for batch in dataloader:\n",
    "                x, labels_in_batch = batch\n",
    "                x = x.view(-1, input_dim)  # 确保x的形状\n",
    "                logits = encoder(x)\n",
    "                z = gumbel_softmax(logits, temperature)\n",
    "                x_recon = decoder(z)\n",
    "\n",
    "                # calculate the losses\n",
    "                try:\n",
    "                    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / x.shape[0]\n",
    "                except:\n",
    "                    break\n",
    "                log_softmax_logits = F.log_softmax(logits, dim=-1)\n",
    "                uniform_distribution = torch.ones_like(log_softmax_logits) * (1.0 / categorical_dim)\n",
    "                one_hot_labels = torch.zeros_like(log_softmax_logits).scatter_(1, labels_in_batch.unsqueeze(1), 1.0)\n",
    "                kl_loss = F.kl_div(log_softmax_logits, one_hot_labels, reduction='sum')\n",
    "                loss = recon_loss + kl_loss\n",
    "\n",
    "                # gradient descent & backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # log the losses\n",
    "                recon_losses.append(recon_loss.item())\n",
    "                kl_losses.append(kl_loss.item())\n",
    "                total_losses.append(loss.item())\n",
    "                lr_list.append(scheduler.get_last_lr()[0])\n",
    "                temperature_list.append(temperature)\n",
    "\n",
    "                # update the progress bar\n",
    "                progress_bar.set_description(f'Epoch: {epoch+1}/{epochs} | Re.Loss: {recon_loss.item():.7f}, KL.Loss: {kl_loss.item():.7f} | Lr: {scheduler.get_last_lr()[0]:.7f} Temp: {temperature:.7f}', refresh=True)\n",
    "                progress_bar.update(1)\n",
    "\n",
    "                scheduler.step()  # update the learning rate\n",
    "                temperature -= annealing_rate # update the temperature\n",
    "                temperature = max(temperature, final_temperature)  # make sure the temperature is not lower than the minimum value\n",
    "\n",
    "            # scheduler.step()  \n",
    "            # temperature -= annealing_rate \n",
    "            # temperature = max(temperature, final_temperature)  \n",
    "            profiler.step()  # Step after each epoch\n",
    "\n",
    "    progress_bar.close()\n",
    "    final_loss = total_losses[-1]\n",
    "\n",
    "    # plot the losses curve\n",
    "    plt.plot(recon_losses, label='Reconstruction Loss')\n",
    "    plt.plot(kl_losses, label='KL Divergence Loss')\n",
    "    plt.plot(total_losses, label='Total Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot the lr curve\n",
    "    plt.plot(lr_list, label='lr')\n",
    "    plt.show()\n",
    "\n",
    "    # plot the temperature curve\n",
    "    plt.plot(temperature_list, label='Temperature')\n",
    "    plt.show()\n",
    "\n",
    "    return dataset, encoder, decoder, final_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def visualize_images(images, num_classes, title):\n",
    "    fig, axes = plt.subplots(1, num_classes, figsize=(num_classes, 1))\n",
    "    for i in range(num_classes):\n",
    "        axes[i].imshow(images[i][0], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    # plt.subplots_adjust(wspace=0.2, hspace=0.8)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def sample_and_reconstruct(dataset, encoder, decoder, input_dim, num_classes, temperature):\n",
    "    samples_per_class = 1\n",
    "    original_images = [[] for _ in range(num_classes)]\n",
    "    reconstructed_images = [[] for _ in range(num_classes)]\n",
    "    selected_classes = [False] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in random.sample(list(dataset), len(dataset)): # Shuffle dataset\n",
    "            if all(selected_classes):\n",
    "                break\n",
    "            if selected_classes[y]:\n",
    "                continue\n",
    "            x = x.view(-1, input_dim)\n",
    "            logits = encoder(x)\n",
    "            z = gumbel_softmax(logits, temperature)\n",
    "            x_recon = decoder(z)\n",
    "            original_images[y].append(x.view(28, 28).numpy())\n",
    "            reconstructed_images[y].append(x_recon.view(28, 28).numpy())\n",
    "            selected_classes[y] = True\n",
    "\n",
    "    visualize_images(original_images, num_classes, 'Original Images')\n",
    "    visualize_images(reconstructed_images, num_classes, 'Reconstructed Images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from latent vector\n",
    "# categorical_dim = 10\n",
    "# input_dim = 28 * 28\n",
    "# temperature = 0.5\n",
    "\n",
    "def generate_from_latent(dataset, encoder, decoder, categorical_dim, temperature):\n",
    "    # init\n",
    "    latent_sums = torch.zeros(categorical_dim, categorical_dim)\n",
    "    class_counts = torch.zeros(categorical_dim, dtype=torch.int)\n",
    "\n",
    "    # calculate the sum of latent vectors for each class\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataset:\n",
    "            img = img.view(-1, input_dim)\n",
    "            logits = encoder(img)\n",
    "            z = gumbel_softmax(logits, temperature).squeeze()\n",
    "            latent_sums[label] += z\n",
    "            class_counts[label] += 1\n",
    "\n",
    "    # calculate the average latent vector for each class\n",
    "    avg_latent_vector = latent_sums / class_counts[:, None]\n",
    "\n",
    "    fig, axes = plt.subplots(1, categorical_dim, figsize=(categorical_dim, 1))\n",
    "\n",
    "    # plot for each class\n",
    "    for i in range(categorical_dim):\n",
    "        generated_img = decoder(avg_latent_vector[i])\n",
    "        generated_img = generated_img.view(28, 28).detach().cpu().numpy()  # 转换为numpy数组\n",
    "        ax = axes[i]\n",
    "        ax.imshow(generated_img, cmap='gray')\n",
    "        ax.set_title(f'Class {i}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # apply transformations to EMNIST images\n",
    "    if dataset_name == 'EMNIST':\n",
    "        from scipy.ndimage import rotate\n",
    "        plt.close(fig)  # 关闭原始图像\n",
    "        rows = 5\n",
    "        cols = 10\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "        # fig.subplots_adjust(hspace=0.2, wspace=0.2)  # 调整子图间的间距\n",
    "        fig.suptitle('Generated EMNIST Images (rotated and flipped)')\n",
    "\n",
    "        for i in range(categorical_dim):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            generated_img = decoder(avg_latent_vector[i])\n",
    "            generated_img = generated_img.view(28, 28).detach().cpu().numpy()\n",
    "            generated_img = rotate(generated_img, -90)  # 逆时针旋转90度\n",
    "            generated_img = np.fliplr(generated_img)  # 水平镜像\n",
    "            ax = axes[row, col]\n",
    "            ax.imshow(generated_img, cmap='gray')\n",
    "            ax.set_title(f'Class {i}')\n",
    "            ax.axis('off')\n",
    "        # 隐藏多余的子图\n",
    "        for i in range(categorical_dim, rows * cols):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            axes[row, col].axis('off')\n",
    "        fig.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "def visualize_latent(dataset_name, encoder, num_classes=10, temperature=1.0):\n",
    "    # init\n",
    "    latent_vectors = torch.empty(0, categorical_dim)\n",
    "    Y = torch.empty(0, dtype=torch.long)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # send dataset to encoder\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.view(-1, input_dim)\n",
    "            logits = encoder(x)\n",
    "            z = gumbel_softmax(logits, temperature)\n",
    "            latent_vectors = torch.cat([latent_vectors, z])\n",
    "            Y = torch.cat([Y, y])\n",
    "\n",
    "    # convert latent vectors to numpy arrays\n",
    "    latent_vectors = latent_vectors.cpu().numpy()\n",
    "    Y = Y.cpu().numpy()\n",
    "\n",
    "    # run t-SNE on latent vectors to get 2D embedding\n",
    "    tsne = TSNE(n_components=2)\n",
    "    latent_vectors_2d = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "    # plot 2D embedding\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    scatter = plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], c=Y, cmap='tab10')\n",
    "    if dataset_name == 'EMNIST':\n",
    "        scatter = plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], c=Y, cmap='tab20')\n",
    "    plt.colorbar(scatter, label='Class Labels')\n",
    "    for i in range(num_classes):\n",
    "        centroid = np.mean(latent_vectors_2d[Y == i], axis=0)\n",
    "        plt.text(centroid[0], centroid[1], str(i), color='black', fontsize=12, fontweight='bold', ha='center', va='center')\n",
    "    plt.show()\n",
    "\n",
    "    # cm\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    # find the most likely label for each test input\n",
    "    predicted_labels = latent_vectors.argmax(axis=1)\n",
    "\n",
    "    # calculate the confusion matrix\n",
    "    cm = confusion_matrix(Y, predicted_labels)\n",
    "\n",
    "    # plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if dataset_name == 'EMNIST':\n",
    "        plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    # calculate accuracy\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    rand_index = adjusted_rand_score(Y, predicted_labels)\n",
    "    print(\"Rand index: \", rand_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-04 06:18:07,072] A new study created in memory with name: no-name-8a5ae8ea-387c-4e07-8cfc-5d4a992c265f\n",
      "Epoch: 1/14 | Re.Loss: 543.2833252, KL.Loss: 248.9413147 | Lr: 0.0082567 Temp: 1.4275788:   2%|▏         | 177/7784 [00:04<03:08, 40.38it/s][W 2023-08-04 06:18:14,937] Trial 0 failed with parameters: {'hidden_dim_1': 337, 'hidden_dim_2': 69, 'batch_size': 108, 'epochs': 14, 'initial_lr': 0.008425440758973004, 'temperature': 1.455846740564253, 'final_temperature': 0.20563387730257987} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/YUAN/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_63561/3017120530.py\", line 16, in objective\n",
      "    dataset, encoder, decoder, final_loss = train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_63561/1614456434.py\", line 70, in train_and_plot\n",
      "    loss.backward()\n",
      "  File \"/Users/YUAN/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/YUAN/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2023-08-04 06:18:14,941] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m final_loss\n\u001b[1;32m     21\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials))\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     13\u001b[0m temperature \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39msuggest_float(\u001b[39m'\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m2.0\u001b[39m)\n\u001b[1;32m     14\u001b[0m final_temperature \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39msuggest_float(\u001b[39m'\u001b[39m\u001b[39mfinal_temperature\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m dataset, encoder, decoder, final_loss \u001b[39m=\u001b[39m train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinished trail \u001b[39m\u001b[39m{\u001b[39;00mcurrent_trial\u001b[39m}\u001b[39;00m\u001b[39m/100 | Final loss: \u001b[39m\u001b[39m{\u001b[39;00mfinal_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m current_trial \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 70\u001b[0m, in \u001b[0;36mtrain_and_plot\u001b[0;34m(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m# gradient descent & backpropagation\u001b[39;00m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 70\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     71\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m \u001b[39m# log the losses\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/14 | Re.Loss: 543.2833252, KL.Loss: 248.9413147 | Lr: 0.0082567 Temp: 1.4275788:   2%|▏         | 177/7784 [00:20<03:08, 40.38it/s]"
     ]
    }
   ],
   "source": [
    "# 超参数调优\n",
    "import optuna\n",
    "current_trial = 0\n",
    "\n",
    "def objective(trial):\n",
    "    global current_trial\n",
    "    dataset_name = 'MNIST'\n",
    "    hidden_dim_1 = trial.suggest_int('hidden_dim_1', 128, 512)\n",
    "    hidden_dim_2 = trial.suggest_int('hidden_dim_2', 32, 128)\n",
    "    batch_size = trial.suggest_int('batch_size', 10, 128)\n",
    "    epochs = trial.suggest_int('epochs', 10, 128)\n",
    "    initial_lr = trial.suggest_float('initial_lr', 1e-4, 1e-2, log=True)\n",
    "    temperature = trial.suggest_float('temperature', 0.5, 2.0)\n",
    "    final_temperature = trial.suggest_float('final_temperature', 0.1, 1.0)\n",
    "    \n",
    "    dataset, encoder, decoder, final_loss = train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)\n",
    "    print(f'Finished trail {current_trial}/100 | Final loss: {final_loss}')\n",
    "    current_trial += 1\n",
    "    return final_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Value: ', trial.value)\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'MNIST'\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 256\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "initial_lr = 1e-3\n",
    "temperature = 1.0\n",
    "final_temperature = 0.3\n",
    "\n",
    "dataset, encoder, decoder, final_loss = train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_reconstruct(dataset, encoder, decoder, input_dim=28*28, num_classes=10, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dim = 10\n",
    "input_dim = 28 * 28\n",
    "temperature = 0.5\n",
    "generate_from_latent(dataset, encoder, decoder, categorical_dim=10, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_latent(dataset_name, encoder, num_classes=10, temperature=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
