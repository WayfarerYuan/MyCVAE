{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c399e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import struct\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Constants for the models\n",
    "input_dim = 28 * 28\n",
    "hidden_dim_1 = 256\n",
    "hidden_dim_2 = 128\n",
    "categorical_dim = 10  # Number of categories in the categorical distribution (One for each digit)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, categorical_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, categorical_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Define the Decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, categorical_dim, hidden_dim_2, hidden_dim_1, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(categorical_dim, hidden_dim_2)\n",
    "        self.fc2 = nn.Linear(hidden_dim_2, hidden_dim_1)\n",
    "        self.fc3 = nn.Linear(hidden_dim_1, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x.view(-1, 1, 28, 28)\n",
    "\n",
    "# Signalling game loss\n",
    "def signalling_game_loss(original_image, decoded_image):\n",
    "    return F.mse_loss(decoded_image, original_image)\n",
    "\n",
    "# Growth rule\n",
    "def grow_population(senders, receivers):\n",
    "    # New sender inherits strategy from a random existing sender with potential small mutation (omitted for simplicity here)\n",
    "    new_sender = Encoder(input_dim, hidden_dim_1, hidden_dim_2, categorical_dim).to(device)\n",
    "    new_sender.load_state_dict(random.choice(senders).state_dict())\n",
    "    \n",
    "    # New receiver inherits strategy from a random existing receiver with potential small mutation (omitted for simplicity here)\n",
    "    new_receiver = Decoder(categorical_dim, hidden_dim_2, hidden_dim_1, input_dim).to(device)\n",
    "    new_receiver.load_state_dict(random.choice(receivers).state_dict())\n",
    "    \n",
    "    senders.append(new_sender)\n",
    "    receivers.append(new_receiver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d94c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9481\n",
      "Epoch 2/10, Loss: 0.9253\n",
      "Epoch 3/10, Loss: 0.9253\n",
      "Epoch 4/10, Loss: 0.9253\n",
      "Epoch 5/10, Loss: 0.9253\n",
      "Epoch 6/10, Loss: 0.9253\n",
      "Epoch 7/10, Loss: 0.9253\n",
      "Epoch 8/10, Loss: 0.9253\n",
      "Epoch 9/10, Loss: 0.9253\n",
      "Epoch 10/10, Loss: 0.9253\n"
     ]
    }
   ],
   "source": [
    "# Constants for pre-training\n",
    "pretrain_epochs = 10\n",
    "pretrain_batch_size = 64\n",
    "pretrain_learning_rate = 0.001\n",
    "\n",
    "# Create data loader for pre-training\n",
    "# Data loaders for the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "pretrain_loader = DataLoader(train_dataset, batch_size=pretrain_batch_size, shuffle=True)\n",
    "\n",
    "# Initialize a single Encoder and Decoder for pre-training\n",
    "encoder_pretrain = Encoder(input_dim, hidden_dim_1, hidden_dim_2, categorical_dim).to(device)\n",
    "decoder_pretrain = Decoder(categorical_dim, hidden_dim_2, hidden_dim_1, input_dim).to(device)\n",
    "\n",
    "# Define optimizer and loss function for pre-training\n",
    "optimizer_pretrain = optim.Adam(list(encoder_pretrain.parameters()) + list(decoder_pretrain.parameters()), lr=pretrain_learning_rate)\n",
    "\n",
    "# Pre-training loop\n",
    "for epoch in range(pretrain_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, _) in enumerate(pretrain_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer_pretrain.zero_grad()\n",
    "        \n",
    "        # Forward pass: Encode and Decode\n",
    "        encoded_data = encoder_pretrain(data)\n",
    "        decoded_data = decoder_pretrain(encoded_data)\n",
    "        \n",
    "        # Calculate loss and backpropagate\n",
    "        loss = signalling_game_loss(data, decoded_data)\n",
    "        loss.backward()\n",
    "        optimizer_pretrain.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    average_loss = total_loss / len(pretrain_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{pretrain_epochs}, Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e4e15c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_33012/2139423504.py:51: UserWarning: Using a target size (torch.Size([1, 28, 28])) that is different to the input size (torch.Size([1, 1, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(decoded_image, original_image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1, Success Rate: 0.0000, Population Size: 4\n",
      "Generation 2, Success Rate: 0.0000, Population Size: 5\n",
      "Generation 3, Success Rate: 0.0000, Population Size: 6\n",
      "Generation 4, Success Rate: 0.0000, Population Size: 7\n",
      "Generation 5, Success Rate: 0.0000, Population Size: 8\n",
      "Generation 6, Success Rate: 0.0000, Population Size: 9\n",
      "Generation 7, Success Rate: 0.0000, Population Size: 10\n",
      "Generation 8, Success Rate: 0.0000, Population Size: 11\n",
      "Generation 9, Success Rate: 0.0000, Population Size: 12\n",
      "Generation 10, Success Rate: 0.0000, Population Size: 13\n",
      "Generation 11, Success Rate: 0.0000, Population Size: 14\n",
      "Generation 12, Success Rate: 0.0000, Population Size: 15\n",
      "Generation 13, Success Rate: 0.0000, Population Size: 16\n",
      "Generation 14, Success Rate: 0.0000, Population Size: 17\n",
      "Generation 15, Success Rate: 0.0000, Population Size: 18\n",
      "Generation 16, Success Rate: 0.0000, Population Size: 19\n",
      "Generation 17, Success Rate: 0.0000, Population Size: 20\n",
      "Generation 18, Success Rate: 0.0000, Population Size: 21\n",
      "Generation 19, Success Rate: 0.0000, Population Size: 22\n",
      "Generation 20, Success Rate: 0.0000, Population Size: 23\n",
      "Generation 21, Success Rate: 0.0000, Population Size: 24\n",
      "Generation 22, Success Rate: 0.0000, Population Size: 25\n",
      "Generation 23, Success Rate: 0.0000, Population Size: 26\n",
      "Generation 24, Success Rate: 0.0000, Population Size: 27\n",
      "Generation 25, Success Rate: 0.0000, Population Size: 28\n",
      "Generation 26, Success Rate: 0.0000, Population Size: 29\n",
      "Generation 27, Success Rate: 0.0000, Population Size: 30\n",
      "Generation 28, Success Rate: 0.0000, Population Size: 31\n",
      "Generation 29, Success Rate: 0.0000, Population Size: 32\n",
      "Generation 30, Success Rate: 0.0000, Population Size: 33\n",
      "Generation 31, Success Rate: 0.0000, Population Size: 34\n",
      "Generation 32, Success Rate: 0.0000, Population Size: 35\n",
      "Generation 33, Success Rate: 0.0000, Population Size: 36\n",
      "Generation 34, Success Rate: 0.0000, Population Size: 37\n",
      "Generation 35, Success Rate: 0.0000, Population Size: 38\n",
      "Generation 36, Success Rate: 0.0000, Population Size: 39\n",
      "Generation 37, Success Rate: 0.0000, Population Size: 40\n",
      "Generation 38, Success Rate: 0.0000, Population Size: 41\n",
      "Generation 39, Success Rate: 0.0000, Population Size: 42\n",
      "Generation 40, Success Rate: 0.0000, Population Size: 43\n",
      "Generation 41, Success Rate: 0.0000, Population Size: 44\n",
      "Generation 42, Success Rate: 0.0000, Population Size: 45\n",
      "Generation 43, Success Rate: 0.0000, Population Size: 46\n",
      "Generation 44, Success Rate: 0.0000, Population Size: 47\n",
      "Generation 45, Success Rate: 0.0000, Population Size: 48\n",
      "Generation 46, Success Rate: 0.0000, Population Size: 49\n",
      "Generation 47, Success Rate: 0.0000, Population Size: 50\n",
      "Generation 48, Success Rate: 0.0000, Population Size: 51\n",
      "Generation 49, Success Rate: 0.0000, Population Size: 52\n",
      "Generation 50, Success Rate: 0.0000, Population Size: 53\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dynamic population interaction and evolution\n",
    "# Constants for data loading\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # Set batch size to 1 for manual interaction\n",
    "\n",
    "# Constants for the dynamic interaction\n",
    "initial_population = 3\n",
    "num_generations_dynamic = 50\n",
    "num_interactions_dynamic = 100\n",
    "\n",
    "# Initialize a small population of agents\n",
    "senders_population = [Encoder(input_dim, hidden_dim_1, hidden_dim_2, categorical_dim).to(device) for _ in range(initial_population)]\n",
    "receivers_population = [Decoder(categorical_dim, hidden_dim_2, hidden_dim_1, input_dim).to(device) for _ in range(initial_population)]\n",
    "\n",
    "# Reset the performance and population size tracking\n",
    "performance_over_generations_dynamic = []\n",
    "population_size_over_generations = []\n",
    "\n",
    "# Main loop for generations with dynamic population growth\n",
    "for generation in range(num_generations_dynamic):\n",
    "    successful_interactions = 0\n",
    "    \n",
    "    # Dynamic interactions within a generation\n",
    "    for interaction in range(num_interactions_dynamic):\n",
    "        # Randomly select a sender and a receiver from the current population\n",
    "        selected_sender = random.choice(senders_population)\n",
    "        selected_receiver = random.choice(receivers_population)\n",
    "        \n",
    "        # Sample a random image\n",
    "        sample_image, _ = next(iter(train_loader))\n",
    "        sample_image = sample_image[0].to(device)\n",
    "        \n",
    "        # Sender encodes the image\n",
    "        signal = selected_sender(sample_image.unsqueeze(0))\n",
    "        \n",
    "        # Receiver decodes the signal\n",
    "        decoded_image = selected_receiver(signal)\n",
    "        \n",
    "        # Check if the communication was successful\n",
    "        loss = signalling_game_loss(sample_image, decoded_image)\n",
    "        if loss < 0.1:  # Using a threshold to determine success\n",
    "            successful_interactions += 1\n",
    "            \n",
    "        # Update the selected sender and receiver based on the result\n",
    "        optimizer_selected = optim.Adam(list(selected_sender.parameters()) + list(selected_receiver.parameters()), lr=learning_rate)\n",
    "        optimizer_selected.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_selected.step()\n",
    "    \n",
    "    # Store the performance for this generation\n",
    "    success_rate = successful_interactions / num_interactions_dynamic\n",
    "    performance_over_generations_dynamic.append(success_rate)\n",
    "    \n",
    "    # Apply the growth rule to increase the population\n",
    "    grow_population(senders_population, receivers_population)\n",
    "    \n",
    "    # Track the population size\n",
    "    population_size = len(senders_population)  # or len(receivers_population), they are the same\n",
    "    population_size_over_generations.append(population_size)\n",
    "    print(f\"Generation {generation + 1}, Success Rate: {success_rate:.4f}, Population Size: {population_size}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myCVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
