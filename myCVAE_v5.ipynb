{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from prodigyopt import Prodigy\n",
    "\n",
    "# Define the model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, categorical_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, categorical_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, categorical_dim, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(categorical_dim, hidden_dim_2)\n",
    "        self.fc2 = nn.Linear(hidden_dim_2, hidden_dim_1)\n",
    "        self.fc3 = nn.Linear(hidden_dim_1, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # 确保输出在[0,1]范围内\n",
    "        return x\n",
    "\n",
    "def gumbel_softmax(logits, temperature):\n",
    "    gs = F.gumbel_softmax(logits, tau=temperature, hard=False, dim=-1)\n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature):\n",
    "    # 加载数据\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "    if dataset_name == \"MNIST\":\n",
    "        dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    elif dataset_name == \"FashionMNIST\":\n",
    "        dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "    elif dataset_name == \"EMNIST\":\n",
    "        dataset = datasets.EMNIST('./data', train=True, download=True, transform=transform, split='balanced')\n",
    "    dataset_img_size = dataset[0][0].shape[0]\n",
    "    # get the number of classes\n",
    "    classes = []\n",
    "    for _, label in dataset:\n",
    "        if label not in classes:\n",
    "            classes.append(label)\n",
    "    num_classes = len(classes)\n",
    "    categorical_dim = num_classes\n",
    "    input_dim = output_dim = dataset_img_size\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    annealing_rate = (temperature - final_temperature) / (epochs * len(dataloader))\n",
    "\n",
    "    # 创建模型\n",
    "    encoder = Encoder(input_dim, hidden_dim_1, hidden_dim_2, categorical_dim)\n",
    "    decoder = Decoder(categorical_dim, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    # optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=initial_lr)\n",
    "    # optimizer = Prodigy(list(encoder.parameters()) + list(decoder.parameters()), lr=initial_lr)\n",
    "    optimizer = optim.AdamW(list(encoder.parameters()) + list(decoder.parameters()), lr=initial_lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=(len(dataloader) * epochs), eta_min=1e-5)\n",
    "\n",
    "    # initialize the loss lists\n",
    "    recon_losses = []\n",
    "    kl_losses = []\n",
    "    total_losses = []\n",
    "    lr_list = []\n",
    "    temperature_list = []\n",
    "\n",
    "    # initialize the progress bar\n",
    "    progress_bar = tqdm(total=(len(dataloader) * epochs), desc=\"Training Progress\")\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            x, labels_in_batch = batch\n",
    "            x = x.view(-1, input_dim)  # 确保x的形状\n",
    "            logits = encoder(x)\n",
    "            z = gumbel_softmax(logits, temperature)\n",
    "            x_recon = decoder(z)\n",
    "\n",
    "            # calculate the losses\n",
    "            try:\n",
    "                recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / x.shape[0]\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                # print('------------------ Error ------------------')\n",
    "                # print(f'x_recon: {x_recon}, \\nx: {x}, \\nx_recon.shape: {x_recon.shape}, x.shape: {x.shape}\\n')\n",
    "                # print(f'logits: {logits}, \\nz: {z}, \\nlogits.shape: {logits.shape}, z.shape: {z.shape}\\n')\n",
    "                # print('------------------ Error ------------------')\n",
    "                break\n",
    "            log_softmax_logits = F.log_softmax(logits, dim=-1)\n",
    "            # uniform_distribution = torch.ones_like(log_softmax_logits) * (1.0 / categorical_dim)\n",
    "            one_hot_labels = torch.zeros_like(log_softmax_logits).scatter_(1, labels_in_batch.unsqueeze(1), 1.0)\n",
    "            kl_loss = F.kl_div(log_softmax_logits, one_hot_labels, reduction='sum') * 18.833685199272043\n",
    "            loss = recon_loss + kl_loss\n",
    "\n",
    "            # gradient descent & backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # log the losses\n",
    "            recon_losses.append(recon_loss.item())\n",
    "            kl_losses.append(kl_loss.item())\n",
    "            total_losses.append(loss.item())\n",
    "            lr_list.append(scheduler.get_last_lr()[0])\n",
    "            temperature_list.append(temperature)\n",
    "\n",
    "            # update the progress bar\n",
    "            progress_bar.set_description(f'Epoch: {epoch+1}/{epochs} | Re.Loss: {recon_loss.item():.7f}, KL.Loss: {kl_loss.item():.7f} | Lr: {scheduler.get_last_lr()[0]:.7f} Temp: {temperature:.7f}', refresh=True)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            scheduler.step()  # update the learning rate\n",
    "            temperature -= annealing_rate # update the temperature\n",
    "            temperature = max(temperature, final_temperature)  # make sure the temperature is not lower than the minimum value\n",
    "\n",
    "        # scheduler.step()  \n",
    "        # temperature -= annealing_rate \n",
    "        # temperature = max(temperature, final_temperature)  \n",
    "\n",
    "    progress_bar.close()\n",
    "    final_loss = total_losses[-1]\n",
    "\n",
    "    # plot the losses curve\n",
    "    plt.plot(recon_losses, label='Reconstruction Loss')\n",
    "    plt.plot(kl_losses, label='KL Divergence Loss')\n",
    "    plt.plot(total_losses, label='Total Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot the lr curve\n",
    "    plt.plot(lr_list, label='lr')\n",
    "    plt.show()\n",
    "\n",
    "    # plot the temperature curve\n",
    "    plt.plot(temperature_list, label='Temperature')\n",
    "    plt.show()\n",
    "\n",
    "    return dataset, encoder, decoder, final_loss\n",
    "\n",
    "# dataset_name = \"MNIST\"  # MNIST, FashionMNIST, EMNIST\n",
    "# # 加载数据\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "# if dataset_name == \"MNIST\":\n",
    "#     dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "# elif dataset_name == \"FashionMNIST\":\n",
    "#     dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "# elif dataset_name == \"EMNIST\":\n",
    "#     dataset = datasets.EMNIST('./data', train=True, download=True, transform=transform, split='balanced')\n",
    "# dataset_img_size = dataset[0][0].shape[0]\n",
    "# # get the number of classes\n",
    "# classes = []\n",
    "# for _, label in dataset:\n",
    "#     if label not in classes:\n",
    "#         classes.append(label)\n",
    "# num_classes = len(classes)\n",
    "\n",
    "\n",
    "# # 超参数\n",
    "# input_dim = dataset_img_size\n",
    "# hidden_dim_1 = 512\n",
    "# hidden_dim_2 = 256\n",
    "# categorical_dim = num_classes\n",
    "# output_dim = input_dim\n",
    "# temperature = 1.0\n",
    "# batch_size = 32\n",
    "# epochs = 64\n",
    "\n",
    "# initial_lr = 1e-3\n",
    "# initial_temperature = 1.0\n",
    "# final_temperature = 0.3\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# annealing_rate = (initial_temperature - final_temperature) / (epochs * len(dataloader))\n",
    "\n",
    "# # 创建模型\n",
    "# encoder = Encoder(input_dim, hidden_dim_1, hidden_dim_2, categorical_dim)\n",
    "# decoder = Decoder(categorical_dim, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "\n",
    "# # Optimizer & Scheduler\n",
    "# # optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=initial_lr)\n",
    "# optimizer = Prodigy(list(encoder.parameters()) + list(decoder.parameters()), lr=initial_lr)\n",
    "# # scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.1)  # T_max设置为周期长度\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=(len(dataloader) * epochs / 4), eta_min=1e-5)\n",
    "\n",
    "# # initialize the loss lists\n",
    "# recon_losses = []\n",
    "# kl_losses = []\n",
    "# total_losses = []\n",
    "# lr_list = []\n",
    "# temperature_list = []\n",
    "\n",
    "# # initialize the progress bar\n",
    "# progress_bar = tqdm(total=(len(dataloader) * epochs), desc=\"Training Progress\")\n",
    "\n",
    "# # training\n",
    "# for epoch in range(epochs):\n",
    "#     for batch in dataloader:\n",
    "#         x, labels_in_batch = batch\n",
    "#         x = x.view(-1, input_dim)  # 确保x的形状\n",
    "#         logits = encoder(x)\n",
    "#         z = gumbel_softmax(logits, temperature)\n",
    "#         x_recon = decoder(z)\n",
    "\n",
    "#         # calculate the losses\n",
    "#         try:\n",
    "#             recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / x.shape[0]\n",
    "#         except:\n",
    "#             # print('------------------ Error ------------------')\n",
    "#             # print(f'x_recon: {x_recon}, \\nx: {x}, \\nx_recon.shape: {x_recon.shape}, x.shape: {x.shape}\\n')\n",
    "#             # print(f'logits: {logits}, \\nz: {z}, \\nlogits.shape: {logits.shape}, z.shape: {z.shape}\\n')\n",
    "#             # print('------------------ Error ------------------')\n",
    "#             break\n",
    "#         log_softmax_logits = F.log_softmax(logits, dim=-1)\n",
    "#         uniform_distribution = torch.ones_like(log_softmax_logits) * (1.0 / categorical_dim)\n",
    "#         one_hot_labels = torch.zeros_like(log_softmax_logits).scatter_(1, labels_in_batch.unsqueeze(1), 1.0)\n",
    "#         kl_loss = F.kl_div(log_softmax_logits, one_hot_labels, reduction='sum')\n",
    "#         loss = recon_loss + kl_loss\n",
    "\n",
    "#         # gradient descent & backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # log the losses\n",
    "#         recon_losses.append(recon_loss.item())\n",
    "#         kl_losses.append(kl_loss.item())\n",
    "#         total_losses.append(loss.item())\n",
    "#         lr_list.append(scheduler.get_last_lr()[0])\n",
    "#         temperature_list.append(temperature)\n",
    "\n",
    "#         # update the progress bar\n",
    "#         progress_bar.set_description(f'Epoch: {epoch+1}/{epochs} | Re.Loss: {recon_loss.item():.7f}, KL.Loss: {kl_loss.item():.7f} | Lr: {scheduler.get_last_lr()[0]:.7f} Temp: {temperature:.7f}', refresh=True)\n",
    "#         progress_bar.update(1)\n",
    "\n",
    "#         scheduler.step()  # update the learning rate\n",
    "#         temperature -= annealing_rate # update the temperature\n",
    "#         temperature = max(temperature, final_temperature)  # make sure the temperature is not lower than the minimum value\n",
    "\n",
    "#     # scheduler.step()  \n",
    "#     # temperature -= annealing_rate \n",
    "#     # temperature = max(temperature, final_temperature)  \n",
    "\n",
    "# progress_bar.close()\n",
    "\n",
    "# # plot the losses curve\n",
    "# plt.plot(recon_losses, label='Reconstruction Loss')\n",
    "# plt.plot(kl_losses, label='KL Divergence Loss')\n",
    "# plt.plot(total_losses, label='Total Loss')\n",
    "# plt.xlabel('Steps')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(lr_list, label='lr')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(temperature_list, label='Temperature')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def visualize_images(images, num_classes, title):\n",
    "    fig, axes = plt.subplots(1, num_classes, figsize=(num_classes, 1))\n",
    "    for i in range(num_classes):\n",
    "        axes[i].imshow(images[i][0], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    # plt.subplots_adjust(wspace=0.2, hspace=0.8)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def sample_and_reconstruct(dataset, encoder, decoder, input_dim, num_classes, temperature):\n",
    "    samples_per_class = 1\n",
    "    original_images = [[] for _ in range(num_classes)]\n",
    "    reconstructed_images = [[] for _ in range(num_classes)]\n",
    "    selected_classes = [False] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in random.sample(list(dataset), len(dataset)): # Shuffle dataset\n",
    "            if all(selected_classes):\n",
    "                break\n",
    "            if selected_classes[y]:\n",
    "                continue\n",
    "            x = x.view(-1, input_dim)\n",
    "            logits = encoder(x)\n",
    "            z = gumbel_softmax(logits, temperature)\n",
    "            x_recon = decoder(z)\n",
    "            original_images[y].append(x.view(28, 28).numpy())\n",
    "            reconstructed_images[y].append(x_recon.view(28, 28).numpy())\n",
    "            selected_classes[y] = True\n",
    "\n",
    "    visualize_images(original_images, num_classes, 'Original Images')\n",
    "    visualize_images(reconstructed_images, num_classes, 'Reconstructed Images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reconstruction from dataset\n",
    "# samples_per_class = 10\n",
    "# original_images = [[] for _ in range(num_classes)]\n",
    "# reconstructed_images = [[] for _ in range(num_classes)]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for x, y in dataset:\n",
    "#         if all(len(images) >= samples_per_class for images in original_images):\n",
    "#             break\n",
    "#         x = x.view(-1, input_dim)\n",
    "#         logits = encoder(x)\n",
    "#         z = gumbel_softmax(logits, temperature)\n",
    "#         x_recon = decoder(z)\n",
    "#         original_images[y].append(x.view(28, 28).numpy())\n",
    "#         reconstructed_images[y].append(x_recon.view(28, 28).numpy())\n",
    "\n",
    "# fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(10, 10))\n",
    "# for i in range(num_classes):\n",
    "#     for j in range(samples_per_class):\n",
    "#         axes[i, j].imshow(original_images[i][j], cmap='gray')\n",
    "#         axes[i, j].axis('off')\n",
    "# plt.suptitle('Original Images')\n",
    "# plt.show()\n",
    "\n",
    "# fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(10, 10))\n",
    "# for i in range(num_classes):\n",
    "#     for j in range(samples_per_class):\n",
    "#         axes[i, j].imshow(reconstructed_images[i][j], cmap='gray')\n",
    "#         axes[i, j].axis('off')\n",
    "# plt.suptitle('Reconstructed Images')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from latent vector\n",
    "# categorical_dim = 10\n",
    "# input_dim = 28 * 28\n",
    "# temperature = 0.5\n",
    "\n",
    "def generate_from_latent(dataset, encoder, decoder, categorical_dim, temperature):\n",
    "    # init\n",
    "    latent_sums = torch.zeros(categorical_dim, categorical_dim)\n",
    "    class_counts = torch.zeros(categorical_dim, dtype=torch.int)\n",
    "\n",
    "    # calculate the sum of latent vectors for each class\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataset:\n",
    "            img = img.view(-1, input_dim)\n",
    "            logits = encoder(img)\n",
    "            z = gumbel_softmax(logits, temperature).squeeze()\n",
    "            latent_sums[label] += z\n",
    "            class_counts[label] += 1\n",
    "\n",
    "    # calculate the average latent vector for each class\n",
    "    avg_latent_vector = latent_sums / class_counts[:, None]\n",
    "\n",
    "    fig, axes = plt.subplots(1, categorical_dim, figsize=(categorical_dim, 1))\n",
    "\n",
    "    # plot for each class\n",
    "    for i in range(categorical_dim):\n",
    "        generated_img = decoder(avg_latent_vector[i])\n",
    "        generated_img = generated_img.view(28, 28).detach().cpu().numpy()  # 转换为numpy数组\n",
    "        ax = axes[i]\n",
    "        ax.imshow(generated_img, cmap='gray')\n",
    "        ax.set_title(f'Class {i}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # apply transformations to EMNIST images\n",
    "    if dataset_name == 'EMNIST':\n",
    "        from scipy.ndimage import rotate\n",
    "        plt.close(fig)  # 关闭原始图像\n",
    "        rows = 5\n",
    "        cols = 10\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "        # fig.subplots_adjust(hspace=0.2, wspace=0.2)  # 调整子图间的间距\n",
    "        fig.suptitle('Generated EMNIST Images (rotated and flipped)')\n",
    "\n",
    "        for i in range(categorical_dim):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            generated_img = decoder(avg_latent_vector[i])\n",
    "            generated_img = generated_img.view(28, 28).detach().cpu().numpy()\n",
    "            generated_img = rotate(generated_img, -90)  # 逆时针旋转90度\n",
    "            generated_img = np.fliplr(generated_img)  # 水平镜像\n",
    "            ax = axes[row, col]\n",
    "            ax.imshow(generated_img, cmap='gray')\n",
    "            ax.set_title(f'Class {i}')\n",
    "            ax.axis('off')\n",
    "        # 隐藏多余的子图\n",
    "        for i in range(categorical_dim, rows * cols):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            axes[row, col].axis('off')\n",
    "        fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "# # init\n",
    "# latent_sums = torch.zeros(categorical_dim, categorical_dim)\n",
    "# class_counts = torch.zeros(categorical_dim, dtype=torch.int)\n",
    "\n",
    "# # calculate the sum of latent vectors for each class\n",
    "# with torch.no_grad():\n",
    "#     for img, label in dataset:\n",
    "#         img = img.view(-1, input_dim)\n",
    "#         logits = encoder(img)\n",
    "#         z = gumbel_softmax(logits, temperature).squeeze()\n",
    "#         latent_sums[label] += z\n",
    "#         class_counts[label] += 1\n",
    "\n",
    "# # calculate the average latent vector for each class\n",
    "# avg_latent_vector = latent_sums / class_counts[:, None]\n",
    "\n",
    "# fig, axes = plt.subplots(1, categorical_dim, figsize=(categorical_dim, 1))\n",
    "\n",
    "# # plot for each class\n",
    "# for i in range(categorical_dim):\n",
    "#     generated_img = decoder(avg_latent_vector[i])\n",
    "#     generated_img = generated_img.view(28, 28).detach().cpu().numpy()  # 转换为numpy数组\n",
    "#     ax = axes[i]\n",
    "#     ax.imshow(generated_img, cmap='gray')\n",
    "#     ax.set_title(f'Class {i}')\n",
    "#     ax.axis('off')\n",
    "\n",
    "# # apply transformations to EMNIST images\n",
    "# if dataset_name == 'EMNIST':\n",
    "#     from scipy.ndimage import rotate\n",
    "#     plt.close(fig)  # 关闭原始图像\n",
    "#     rows = 5\n",
    "#     cols = 10\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "#     # fig.subplots_adjust(hspace=0.2, wspace=0.2)  # 调整子图间的间距\n",
    "#     fig.suptitle('Generated EMNIST Images (rotated and flipped)')\n",
    "\n",
    "#     for i in range(categorical_dim):\n",
    "#         row = i // cols\n",
    "#         col = i % cols\n",
    "#         generated_img = decoder(avg_latent_vector[i])\n",
    "#         generated_img = generated_img.view(28, 28).detach().cpu().numpy()\n",
    "#         generated_img = rotate(generated_img, -90)  # 逆时针旋转90度\n",
    "#         generated_img = np.fliplr(generated_img)  # 水平镜像\n",
    "#         ax = axes[row, col]\n",
    "#         ax.imshow(generated_img, cmap='gray')\n",
    "#         ax.set_title(f'Class {i}')\n",
    "#         ax.axis('off')\n",
    "#     # 隐藏多余的子图\n",
    "#     for i in range(categorical_dim, rows * cols):\n",
    "#         row = i // cols\n",
    "#         col = i % cols\n",
    "#         axes[row, col].axis('off')\n",
    "#     fig.tight_layout()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "def visualize_latent(dataset_name, encoder, num_classes=10, temperature=1.0):\n",
    "    # init\n",
    "    latent_vectors = torch.empty(0, categorical_dim)\n",
    "    Y = torch.empty(0, dtype=torch.long)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # send dataset to encoder\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.view(-1, input_dim)\n",
    "            logits = encoder(x)\n",
    "            z = gumbel_softmax(logits, temperature)\n",
    "            latent_vectors = torch.cat([latent_vectors, z])\n",
    "            Y = torch.cat([Y, y])\n",
    "\n",
    "    # convert latent vectors to numpy arrays\n",
    "    latent_vectors = latent_vectors.cpu().numpy()\n",
    "    Y = Y.cpu().numpy()\n",
    "\n",
    "    # run t-SNE on latent vectors to get 2D embedding\n",
    "    tsne = TSNE(n_components=2)\n",
    "    latent_vectors_2d = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "    # plot 2D embedding\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    scatter = plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], c=Y, cmap='tab10')\n",
    "    if dataset_name == 'EMNIST':\n",
    "        scatter = plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], c=Y, cmap='tab20')\n",
    "    plt.colorbar(scatter, label='Class Labels')\n",
    "    for i in range(num_classes):\n",
    "        centroid = np.mean(latent_vectors_2d[Y == i], axis=0)\n",
    "        plt.text(centroid[0], centroid[1], str(i), color='black', fontsize=12, fontweight='bold', ha='center', va='center')\n",
    "    plt.show()\n",
    "\n",
    "    # cm\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    # find the most likely label for each test input\n",
    "    predicted_labels = latent_vectors.argmax(axis=1)\n",
    "\n",
    "    # calculate the confusion matrix\n",
    "    cm = confusion_matrix(Y, predicted_labels)\n",
    "\n",
    "    # plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if dataset_name == 'EMNIST':\n",
    "        plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    # calculate accuracy\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    rand_index = adjusted_rand_score(Y, predicted_labels)\n",
    "    print(\"Rand index: \", rand_index)\n",
    "\n",
    "# # init\n",
    "# latent_vectors = torch.empty(0, categorical_dim)\n",
    "# Y = torch.empty(0, dtype=torch.long)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # send dataset to encoder\n",
    "# with torch.no_grad():\n",
    "#     for x, y in dataloader:\n",
    "#         x = x.view(-1, input_dim)\n",
    "#         logits = encoder(x)\n",
    "#         z = gumbel_softmax(logits, temperature)\n",
    "#         latent_vectors = torch.cat([latent_vectors, z])\n",
    "#         Y = torch.cat([Y, y])\n",
    "\n",
    "# # convert latent vectors to numpy arrays\n",
    "# latent_vectors = latent_vectors.cpu().numpy()\n",
    "# Y = Y.cpu().numpy()\n",
    "\n",
    "# # run t-SNE on latent vectors to get 2D embedding\n",
    "# tsne = TSNE(n_components=2)\n",
    "# latent_vectors_2d = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "# # plot 2D embedding\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# scatter = plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], c=Y, cmap='tab10')\n",
    "# if dataset_name == 'EMNIST':\n",
    "#     scatter = plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], c=Y, cmap='tab20')\n",
    "# plt.colorbar(scatter, label='Class Labels')\n",
    "# for i in range(num_classes):\n",
    "#     centroid = np.mean(latent_vectors_2d[Y == i], axis=0)\n",
    "#     plt.text(centroid[0], centroid[1], str(i), color='black', fontsize=12, fontweight='bold', ha='center', va='center')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "\n",
    "# # find the most likely label for each test input\n",
    "# predicted_labels = latent_vectors.argmax(axis=1)\n",
    "\n",
    "# # calculate the confusion matrix\n",
    "# cm = confusion_matrix(Y, predicted_labels)\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# if dataset_name == 'EMNIST':\n",
    "#     plt.figure(figsize=(50, 40))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rand-index\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# rand_index = adjusted_rand_score(Y, predicted_labels)\n",
    "# print(\"Rand index: \", rand_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 95/306 | Re.Loss: 100.3103180, KL.Loss: 2.3759885 | Lr: 0.0005596 Temp: 1.4042376:  31%|███       | 331200/1080180 [1:29:55<3:29:38, 59.55it/s]  "
     ]
    }
   ],
   "source": [
    "dataset_name = 'MNIST'\n",
    "# hidden_dim_1 = 512\n",
    "# hidden_dim_2 = 256\n",
    "# batch_size = 32\n",
    "# epochs = 64\n",
    "# initial_lr = 1e-3\n",
    "# temperature = 1.0\n",
    "# final_temperature = 0.3\n",
    "\n",
    "# hidden_dim_1 = 395\n",
    "# hidden_dim_2 = 87\n",
    "# batch_size = 229\n",
    "# epochs = 108\n",
    "# initial_lr = 0.008221946643391686\n",
    "# temperature = 1.8602898465279396\n",
    "# final_temperature = 0.9793508307172389\n",
    "# beta = / batch_size * 29.843531299928905\n",
    "\n",
    "# hidden_dim_1 = 425\n",
    "# hidden_dim_2 = 500\n",
    "# batch_size = 56\n",
    "# epochs = 360\n",
    "# initial_lr = 0.006023831168192716\n",
    "# temperature = 1.4426243165789046\n",
    "# final_temperature = 1.479644293752712\n",
    "\n",
    "# Trial 57: \n",
    "# Hidden Dim 1: 940\n",
    "# Hidden Dim 2: 363\n",
    "# Batch Size: 17\n",
    "# Epochs: 306\n",
    "# Initial LR: 0.0007097821685505025\n",
    "# Temperature: 0.7979487217875562\n",
    "# Final Temperature: 1.2619637882684076\n",
    "# Beta: 18.833685199272043\n",
    "# Final Loss: 74.27774047851562\n",
    "hidden_dim_1 = 940\n",
    "hidden_dim_2 = 363\n",
    "batch_size = 17\n",
    "epochs = 306\n",
    "initial_lr = 0.0007097821685505025\n",
    "temperature = 0.7979487217875562\n",
    "final_temperature = 1.2619637882684076\n",
    "\n",
    "dataset, encoder, decoder, final_loss = train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_reconstruct(dataset, encoder, decoder, input_dim=28*28, num_classes=10, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dim = 10\n",
    "input_dim = 28 * 28\n",
    "temperature = 0.5\n",
    "generate_from_latent(dataset, encoder, decoder, categorical_dim=10, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_latent(dataset_name, encoder, num_classes=10, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on Fashion MNIST dataset\n",
    "dataset_name = 'FashionMNIST'\n",
    "# hidden_dim_1 = 512\n",
    "# hidden_dim_2 = 256\n",
    "# batch_size = 32\n",
    "# epochs = 32\n",
    "# initial_lr = 1e-3\n",
    "# temperature = 1.0\n",
    "# final_temperature = 0.3\n",
    "\n",
    "dataset, encoder, decoder, final_loss = train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_reconstruct(dataset, encoder, decoder, input_dim=28*28, num_classes=10, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_latent(dataset, encoder, decoder, categorical_dim=10, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_latent(dataset_name, encoder, num_classes=10, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on EMNIST dataset\n",
    "dataset_name = 'EMNIST'\n",
    "# hidden_dim_1 = 512\n",
    "# hidden_dim_2 = 256\n",
    "# batch_size = 32\n",
    "# epochs = 32\n",
    "# initial_lr = 1e-3\n",
    "# temperature = 1.0\n",
    "# final_temperature = 0.3\n",
    "\n",
    "dataset, encoder, decoder, final_loss = train_and_plot(dataset_name, hidden_dim_1, hidden_dim_2, batch_size, epochs, initial_lr, temperature, final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_reconstruct(dataset, encoder, decoder, input_dim=28*28, num_classes=47, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dim = 47\n",
    "input_dim = 28 * 28\n",
    "temperature = 0.5\n",
    "generate_from_latent(dataset, encoder, decoder, categorical_dim=47, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_latent(dataset_name, encoder, num_classes=47, temperature=final_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
